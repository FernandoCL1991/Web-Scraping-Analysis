{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n",
      "[WDM] - Driver [C:\\Users\\fernando\\.wdm\\drivers\\chromedriver\\win32\\89.0.4389.23\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splinter setup\n",
    "\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visit the mars nasa news site\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Optional delay for loading the page\n",
    "browser.is_element_present_by_css(\"ul.item_list li.slide\", wait_time=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the browser html to a soup object and then quit the browser\n",
    "html = browser.html\n",
    "news_soup = soup(html, 'html.parser')\n",
    "\n",
    "slide_elem = news_soup.select_one('ul.item_list li.slide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content_title\"><a href=\"/news/8936/nasas-ingenuity-helicopter-to-begin-new-demonstration-phase/\" target=\"_self\">NASA's Ingenuity Helicopter to Begin New Demonstration Phase</a></div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slide_elem.find(\"div\", class_='content_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NASA's Ingenuity Helicopter to Begin New Demonstration Phase\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_title = slide_elem.find(\"div\", class_='content_title').get_text()\n",
    "news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Red Planet rotorcraft will shift focus from proving flight is possible on Mars to demonstrating flight operations that future aerial craft could utilize.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_p = slide_elem.find('div', class_=\"article_teaser_body\").get_text()\n",
    "news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to the result of the scrape: news_title\n",
    "\n",
    "def news_title(browser):\n",
    "    \n",
    "    # Passing url for 'news_title' function\n",
    "    url = 'https://redplanetscience.com/'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Scrapping with Beautiful Soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Try - Except to handle errors while scrapping..\n",
    "    try:\n",
    "        \n",
    "        # Retrieving elements\n",
    "        article = soup.find('div', class_='list_text') \n",
    "        # Creating 'title' variable\n",
    "        news_title = slide_elem.find(\"div\", class_='content_title').get_text()\n",
    "        news_title\n",
    "    \n",
    "    except \"local variable 'title' referenced before assignment\" as e:\n",
    "        \n",
    "        return news_title == None\n",
    "    \n",
    "    except \"catching classes that do not inherit from BaseException is not allowed\" as e:\n",
    "\n",
    "        return news_title == None\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        return news_title == None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return news_title\n",
    "    \n",
    "    finally:\n",
    "        \n",
    "        return news_title\n",
    "    \n",
    "# Closing function difining: news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to the result of the scrape: news_p\n",
    "\n",
    "def news_p(browser):\n",
    "    \n",
    "        # Passing url for 'news_p' function\n",
    "        url = 'https://redplanetscience.com/'\n",
    "        browser.visit(url)\n",
    "\n",
    "        # Scrapping with Beautiful Soup\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "        # Try - Except to handle errors while scrapping..\n",
    "        try:\n",
    "\n",
    "            # Retrieving elements\n",
    "            article = soup.find('div', class_='list_text') \n",
    "            # Creating 'paragraph' variable\n",
    "            news_p = slide_elem.find('div', class_=\"article_teaser_body\").get_text()\n",
    "            news_p\n",
    "            \n",
    "        except \"local variable 'paragraph' referenced before assignment\" as e:\n",
    "            \n",
    "            return news_p == None\n",
    "        \n",
    "        except \"'NoneType' object has no attribute 'find'\" as e:\n",
    "            \n",
    "            return news_p == None             \n",
    "        \n",
    "        except:\n",
    "\n",
    "            return news_p == None\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return news_p\n",
    "\n",
    "# Closing function difining: news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_p(browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JPL Mars Space Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to the result of the scrape: current_img_url\n",
    "\n",
    "def current_img_url(browser):\n",
    "\n",
    "    # Passing url for 'current_img_url' function\n",
    "    url = 'https://spaceimages-mars.com/'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Scrapping with Beautiful Soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Try - Except to handle errors while scrapping..\n",
    "    try:\n",
    "\n",
    "        # Retrieving elements\n",
    "        article = soup.find('div', class_='header')\n",
    "        # Creating 'image_url' variable\n",
    "        image_url = article.find('img', class_= 'headerimage fade-in').get('src')\n",
    "\n",
    "        # Creating final url variable\n",
    "        featured_image_url = f'https://spaceimages-mars.com/{image_url}'\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        return featured_image_url == None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return featured_image_url\n",
    "    \n",
    "# Closing function difining: return featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_img_url(browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to the result of the scrape: mars_facts\n",
    "\n",
    "def mars_facts(browser):\n",
    "\n",
    "    # Passing url for 'mars_facts_df'\n",
    "    url = 'https://galaxyfacts-mars.com/'\n",
    "\n",
    "    # Retrieving elements into 'tables'\n",
    "    tables = pd.read_html(url)\n",
    "    \n",
    "    # Try - Except to handle errors while scrapping..\n",
    "    try:\n",
    "        \n",
    "        # Selecting desired table. Creating 'mars_facts_df' variable\n",
    "        mars_facts_df = tables[0]\n",
    "        mars_facts_df_cols = ['Mars-Earth Comparisson', 'Mars', 'Earth']\n",
    "        mars_facts_df.columns = mars_facts_df_cols\n",
    "        mars_facts_df.drop(labels=[0], axis= 0, inplace=True)\n",
    "        mars_facts_dict = mars_facts_df.to_dict(orient='record')\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        return mars_facts_dict == None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return mars_facts_dict\n",
    "        \n",
    "# Closing function difining: mars_facts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts(browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to the result of the scrape: all_hemispheres_data\n",
    "\n",
    "def all_hemispheres_data(browser):\n",
    "    \n",
    "    # Passing url for 'hemisphere_image_urls'\n",
    "    url = 'https://marshemispheres.com'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Scrapping with Beautiful Soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Try - Except to handle errors while scrapping..\n",
    "    try:\n",
    "\n",
    "        # Total number of hemispheres pages to visit to\n",
    "        links= browser.find_by_css('a.product-item img')\n",
    "\n",
    "        # Creating a list w/ 4 dictionaries with title and link to image per hemisphere w/ Splinter\n",
    "        hemisphere_image_urls  = []\n",
    "\n",
    "        for i in range(len(links)):\n",
    "\n",
    "            # creating dictionary\n",
    "            hemispheres_dict = {}\n",
    "\n",
    "            # first page, click to second\n",
    "            browser.find_by_css('a.product-item img')[i].click()\n",
    "\n",
    "            # on second page, retrieving title   \n",
    "            title= browser.find_by_css('h2').text\n",
    "            hemispheres_dict['title']= title\n",
    "\n",
    "            # on second page, retrieving url\n",
    "            img_url= browser.links.find_by_text('Sample').first['href']\n",
    "            hemispheres_dict['img_url']= img_url\n",
    "\n",
    "            # appending both results to list 'empty_list'\n",
    "            hemisphere_image_urls.append(hemispheres_dict)\n",
    "\n",
    "            # browse back to home page\n",
    "            browser.back()\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        return hemisphere_image_urls == None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return hemisphere_image_urls\n",
    "            \n",
    "# Closing function difining: all_hemispheres_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hemispheres_data(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to the result of the scrape: scrape\n",
    "\n",
    "def scrape(browser):\n",
    "    \n",
    "    data={\n",
    "        \"news_title\": news_title(browser),\n",
    "        \"news_p\": news_p(browser),\n",
    "        \"current_img_url\" : current_img_url(browser),\n",
    "        \"mars_facts\" : mars_facts(browser),\n",
    "        \"all_hemispheres_data\" : all_hemispheres_data(browser)\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
